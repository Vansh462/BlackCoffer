{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-..."
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_excel('input.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL_ID    100\n",
       "URL       100\n",
       "Name: count, dtype: object"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().loc['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def fetch_article(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract the article title (adjust the selector to match the website's structure)\n",
    "    title = soup.find('h1').get_text(strip=True) if soup.find('h1') else 'No Title Found'\n",
    "    \n",
    "    # Extract the article text (adjust the selector to match the website's structure)\n",
    "    paragraphs = soup.find_all('p')\n",
    "    article_text = '\\n'.join([para.get_text(strip=True) for para in paragraphs])\n",
    "    \n",
    "    return title, article_text\n",
    "\n",
    "def save_article(url_id, title, text):\n",
    "    # Define the file name based on URL_ID\n",
    "    file_path = f\"data/{url_id}.txt\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"{title}\\n\\n{text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved article blackassign0001\n",
      "Successfully saved article blackassign0002\n",
      "Successfully saved article blackassign0003\n",
      "Successfully saved article blackassign0004\n",
      "Successfully saved article blackassign0005\n",
      "Successfully saved article blackassign0006\n",
      "Successfully saved article blackassign0007\n",
      "Successfully saved article blackassign0008\n",
      "Successfully saved article blackassign0009\n",
      "Successfully saved article blackassign0010\n",
      "Successfully saved article blackassign0011\n",
      "Successfully saved article blackassign0012\n",
      "Successfully saved article blackassign0013\n",
      "Successfully saved article blackassign0014\n",
      "Successfully saved article blackassign0015\n",
      "Successfully saved article blackassign0016\n",
      "Successfully saved article blackassign0017\n",
      "Successfully saved article blackassign0018\n",
      "Successfully saved article blackassign0019\n",
      "Successfully saved article blackassign0020\n",
      "Successfully saved article blackassign0021\n",
      "Successfully saved article blackassign0022\n",
      "Successfully saved article blackassign0023\n",
      "Successfully saved article blackassign0024\n",
      "Successfully saved article blackassign0025\n",
      "Successfully saved article blackassign0026\n",
      "Successfully saved article blackassign0027\n",
      "Successfully saved article blackassign0028\n",
      "Successfully saved article blackassign0029\n",
      "Successfully saved article blackassign0030\n",
      "Successfully saved article blackassign0031\n",
      "Successfully saved article blackassign0032\n",
      "Successfully saved article blackassign0033\n",
      "Successfully saved article blackassign0034\n",
      "Successfully saved article blackassign0035\n",
      "Failed to process https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Successfully saved article blackassign0037\n",
      "Successfully saved article blackassign0038\n",
      "Successfully saved article blackassign0039\n",
      "Successfully saved article blackassign0040\n",
      "Successfully saved article blackassign0041\n",
      "Successfully saved article blackassign0042\n",
      "Successfully saved article blackassign0043\n",
      "Successfully saved article blackassign0044\n",
      "Successfully saved article blackassign0045\n",
      "Successfully saved article blackassign0046\n",
      "Successfully saved article blackassign0047\n",
      "Successfully saved article blackassign0048\n",
      "Failed to process https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Successfully saved article blackassign0050\n",
      "Successfully saved article blackassign0051\n",
      "Successfully saved article blackassign0052\n",
      "Successfully saved article blackassign0053\n",
      "Successfully saved article blackassign0054\n",
      "Successfully saved article blackassign0055\n",
      "Successfully saved article blackassign0056\n",
      "Successfully saved article blackassign0057\n",
      "Successfully saved article blackassign0058\n",
      "Successfully saved article blackassign0059\n",
      "Successfully saved article blackassign0060\n",
      "Successfully saved article blackassign0061\n",
      "Successfully saved article blackassign0062\n",
      "Successfully saved article blackassign0063\n",
      "Successfully saved article blackassign0064\n",
      "Successfully saved article blackassign0065\n",
      "Successfully saved article blackassign0066\n",
      "Successfully saved article blackassign0067\n",
      "Successfully saved article blackassign0068\n",
      "Successfully saved article blackassign0069\n",
      "Successfully saved article blackassign0070\n",
      "Successfully saved article blackassign0071\n",
      "Successfully saved article blackassign0072\n",
      "Successfully saved article blackassign0073\n",
      "Successfully saved article blackassign0074\n",
      "Successfully saved article blackassign0075\n",
      "Successfully saved article blackassign0076\n",
      "Successfully saved article blackassign0077\n",
      "Successfully saved article blackassign0078\n",
      "Successfully saved article blackassign0079\n",
      "Successfully saved article blackassign0080\n",
      "Successfully saved article blackassign0081\n",
      "Successfully saved article blackassign0082\n",
      "Successfully saved article blackassign0083\n",
      "Successfully saved article blackassign0084\n",
      "Successfully saved article blackassign0085\n",
      "Successfully saved article blackassign0086\n",
      "Successfully saved article blackassign0087\n",
      "Successfully saved article blackassign0088\n",
      "Successfully saved article blackassign0089\n",
      "Successfully saved article blackassign0090\n",
      "Successfully saved article blackassign0091\n",
      "Successfully saved article blackassign0092\n",
      "Successfully saved article blackassign0093\n",
      "Successfully saved article blackassign0094\n",
      "Successfully saved article blackassign0095\n",
      "Successfully saved article blackassign0096\n",
      "Successfully saved article blackassign0097\n",
      "Successfully saved article blackassign0098\n",
      "Successfully saved article blackassign0099\n",
      "Successfully saved article blackassign0100\n"
     ]
    }
   ],
   "source": [
    "for _, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "        \n",
    "    try:\n",
    "        title, text = fetch_article(url)\n",
    "        save_article(url_id, title, text)\n",
    "        print(f\"Successfully saved article {url_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "def load_stopwords(folder_path):\n",
    "    stop_words = set()\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r') as file:\n",
    "                stop_words.update(word.lower() for word in file.read().splitlines())\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "\n",
    "# Function to clean text using stop words\n",
    "def clean_text(text_path):\n",
    "    with open(text_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords from the StopWords folder\n",
    "stop_words = load_stopwords('StopWords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words=set()\n",
    "with open('MasterDictionary/positive-words.txt', 'r') as file:\n",
    "    tokens=nltk.word_tokenize(file.read())\n",
    "for token in tokens:\n",
    "    positive_words.add(token)\n",
    "\n",
    "negative_words=set()\n",
    "with open('MasterDictionary/negative-words.txt', 'r') as file:\n",
    "    tokens=nltk.word_tokenize(file.read())\n",
    "for token in tokens:\n",
    "    negative_words.add(token)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_score(text):\n",
    "    try:\n",
    "        cleaned_text=clean_text(text)\n",
    "        tokens = nltk.word_tokenize(cleaned_text)\n",
    "    \n",
    "        positive_score = sum(1 for word in tokens if word.lower() in positive_words)\n",
    "        return positive_score\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_score(text):\n",
    "    try:\n",
    "        cleaned_text=clean_text(text)\n",
    "        tokens = nltk.word_tokenize(cleaned_text)\n",
    "    \n",
    "        negative_score = sum(1 for word in tokens if word.lower() in negative_words)\n",
    "        return negative_score\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_score(text):\n",
    "    try:\n",
    "        return (positive_score(text)-negative_score(text))/(positive_score(text)+negative_score(text)+0.000001)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjectivity_score(text):\n",
    "    try:\n",
    "        cleaned_text=clean_text(text)\n",
    "        tokens=nltk.word_tokenize(cleaned_text)\n",
    "        Subjectivity_Score = (positive_score(text) + negative_score(text))/ ((len(tokens)) + 0.000001)\n",
    "        return Subjectivity_Score\n",
    "    except:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentence_length(text):\n",
    "    try:\n",
    "        cleaned_text=clean_text(text)\n",
    "        tokens=nltk.word_tokenize(cleaned_text)\n",
    "        nwords=len(tokens)\n",
    "        nsentences=len(cleaned_text.split(\".\"))\n",
    "        return nwords/nsentences\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "\n",
    "    if word.endswith(\"e\"):\n",
    "        word = word[:-1]\n",
    "        \n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    \n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "\n",
    "    if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
    "        count -= 1\n",
    "    \n",
    "    if count == 0:\n",
    "        count = 1\n",
    "        \n",
    "    return count\n",
    "\n",
    "def syllable_per_word(text_path):\n",
    "    try:\n",
    "        with open(text_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        words=nltk.word_tokenize(text)\n",
    "        return sum(syllable_count(word) for word in words)/len(words)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def complex_word_percentage(text_path):\n",
    "    try:\n",
    "        with open(text_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        words = nltk.word_tokenize(text)\n",
    "        complex_words_count = sum(1 for word in words if syllable_count(word) > 2)\n",
    "        total_words = len(words)\n",
    "        percentage = (complex_words_count / total_words) * 100 if total_words > 0 else 0\n",
    "        return percentage\n",
    "    except:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fog_index(text):\n",
    "    try:\n",
    "        fog_index = 0.4 * (average_sentence_length(text) + complex_word_percentage(text))\n",
    "        return fog_index\n",
    "    except:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_word_count(text_path):\n",
    "    try:\n",
    "        with open(text_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        words = nltk.word_tokenize(text)\n",
    "        return sum(1 for word in words if syllable_count(word) > 2)\n",
    "    except:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    try:\n",
    "        cleaned_text=clean_text(text)\n",
    "        tokens=nltk.word_tokenize(cleaned_text)\n",
    "        return len(tokens)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_personal_pronouns(text_path):\n",
    "    try:\n",
    "        with open(text_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        lower_text = text.lower()\n",
    "        pronoun_pattern = r'\\b(i|we|my|ours|us)\\b'\n",
    "        matches = re.findall(pronoun_pattern, lower_text)\n",
    "        filtered_matches = [match for match in matches if not re.search(r'\\bUS\\b', text)]\n",
    "        return len(filtered_matches)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_length(text):\n",
    "    try:\n",
    "        with open(text, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        words = text.split()\n",
    "        total_characters = sum(len(word) for word in words)\n",
    "        total_words = len(words)\n",
    "        if total_words > 0:\n",
    "            avg_word_length = total_characters / total_words\n",
    "        else:\n",
    "            avg_word_length = 0\n",
    "    \n",
    "        return avg_word_length\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "\n",
       "   POSITIVE SCORE NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0             NaN            NaN             NaN                 NaN   \n",
       "1             NaN            NaN             NaN                 NaN   \n",
       "2             NaN            NaN             NaN                 NaN   \n",
       "3             NaN            NaN             NaN                 NaN   \n",
       "4             NaN                            NaN                 NaN   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                  NaN                          NaN        NaN   \n",
       "1                  NaN                          NaN        NaN   \n",
       "2                  NaN                          NaN        NaN   \n",
       "3                  NaN                          NaN        NaN   \n",
       "4                  NaN                          NaN        NaN   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                               NaN                 NaN         NaN   \n",
       "1                               NaN                 NaN         NaN   \n",
       "2                               NaN                 NaN         NaN   \n",
       "3                               NaN                 NaN         NaN   \n",
       "4                               NaN                 NaN         NaN   \n",
       "\n",
       "   SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0                NaN                NaN              NaN  \n",
       "1                NaN                NaN              NaN  \n",
       "2                NaN                NaN              NaN  \n",
       "3                NaN                NaN              NaN  \n",
       "4                NaN                NaN              NaN  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_excel(\"Output Data Structure.xlsx\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['POSITIVE SCORE'] = df1['URL_ID'].apply(lambda url_id: positive_score(f\"data/{url_id}.txt\"))\n",
    "df1['NEGATIVE SCORE'] = df1['URL_ID'].apply(lambda url_id: negative_score(f\"data/{url_id}.txt\"))\n",
    "df1['POLARITY SCORE'] = df1['URL_ID'].apply(lambda url_id: polarity_score(f\"data/{url_id}.txt\"))\n",
    "df1['SUBJECTIVITY SCORE'] = df1['URL_ID'].apply(lambda url_id: subjectivity_score(f\"data/{url_id}.txt\"))\n",
    "df1['AVG SENTENCE LENGTH'] = df1['URL_ID'].apply(lambda url_id: average_sentence_length(f\"data/{url_id}.txt\"))\n",
    "df1['PERCENTAGE OF COMPLEX WORDS'] = df1['URL_ID'].apply(lambda url_id: complex_word_percentage(f\"data/{url_id}.txt\"))\n",
    "df1['FOG INDEX'] = df1['URL_ID'].apply(lambda url_id: fog_index(f\"data/{url_id}.txt\"))\n",
    "df1['AVG NUMBER OF WORDS PER SENTENCE'] = df1['URL_ID'].apply(lambda url_id: average_sentence_length(f\"data/{url_id}.txt\"))\n",
    "df1['COMPLEX WORD COUNT'] = df1['URL_ID'].apply(lambda url_id: complex_word_count(f\"data/{url_id}.txt\"))\n",
    "df1['WORD COUNT'] = df1['URL_ID'].apply(lambda url_id: word_count(f\"data/{url_id}.txt\"))\n",
    "df1['SYLLABLE PER WORD'] = df1['URL_ID'].apply(lambda url_id: syllable_per_word(f\"data/{url_id}.txt\"))\n",
    "df1['PERSONAL PRONOUNS'] = df1['URL_ID'].apply(lambda url_id: count_personal_pronouns(f\"data/{url_id}.txt\"))\n",
    "df1['AVG WORD LENGTH'] = df1['URL_ID'].apply(lambda url_id: average_word_length(f\"data/{url_id}.txt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>11.612903</td>\n",
       "      <td>17.384370</td>\n",
       "      <td>11.598909</td>\n",
       "      <td>11.612903</td>\n",
       "      <td>109</td>\n",
       "      <td>360</td>\n",
       "      <td>1.658692</td>\n",
       "      <td>6</td>\n",
       "      <td>5.397810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>58</td>\n",
       "      <td>30</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.084860</td>\n",
       "      <td>12.345238</td>\n",
       "      <td>19.500272</td>\n",
       "      <td>12.738204</td>\n",
       "      <td>12.345238</td>\n",
       "      <td>359</td>\n",
       "      <td>1037</td>\n",
       "      <td>1.730581</td>\n",
       "      <td>0</td>\n",
       "      <td>5.832913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>44</td>\n",
       "      <td>24</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.073040</td>\n",
       "      <td>15.262295</td>\n",
       "      <td>25.928473</td>\n",
       "      <td>16.476307</td>\n",
       "      <td>15.262295</td>\n",
       "      <td>377</td>\n",
       "      <td>931</td>\n",
       "      <td>1.898900</td>\n",
       "      <td>15</td>\n",
       "      <td>6.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.130577</td>\n",
       "      <td>16.410714</td>\n",
       "      <td>24.653740</td>\n",
       "      <td>16.425782</td>\n",
       "      <td>16.410714</td>\n",
       "      <td>356</td>\n",
       "      <td>919</td>\n",
       "      <td>1.849030</td>\n",
       "      <td>7</td>\n",
       "      <td>6.216238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>19.358074</td>\n",
       "      <td>12.943230</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>193</td>\n",
       "      <td>585</td>\n",
       "      <td>1.725176</td>\n",
       "      <td>8</td>\n",
       "      <td>5.899658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "\n",
       "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0              10               2        0.666667            0.033333   \n",
       "1              58              30        0.318182            0.084860   \n",
       "2              44              24        0.294118            0.073040   \n",
       "3              45              75       -0.250000            0.130577   \n",
       "4              27               9        0.500000            0.061538   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0            11.612903                    17.384370  11.598909   \n",
       "1            12.345238                    19.500272  12.738204   \n",
       "2            15.262295                    25.928473  16.476307   \n",
       "3            16.410714                    24.653740  16.425782   \n",
       "4            13.000000                    19.358074  12.943230   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                         11.612903                 109         360   \n",
       "1                         12.345238                 359        1037   \n",
       "2                         15.262295                 377         931   \n",
       "3                         16.410714                 356         919   \n",
       "4                         13.000000                 193         585   \n",
       "\n",
       "   SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0           1.658692                  6         5.397810  \n",
       "1           1.730581                  0         5.832913  \n",
       "2           1.898900                 15         6.352941  \n",
       "3           1.849030                  7         6.216238  \n",
       "4           1.725176                  8         5.899658  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_excel('output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
